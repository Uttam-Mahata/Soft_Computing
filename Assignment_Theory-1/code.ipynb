{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation: Logistic Regression with Gradient Descent\n",
    "\n",
    "This documentation provides a detailed explanation of a logistic regression implementation using gradient descent for optimization. The code focuses on the training of a logistic regression model, updates the weights and bias using gradient descent, and visualizes the performance through a plot of error and learning rate over iterations.\n",
    "\n",
    "### Code Overview\n",
    "\n",
    "The code is structured into several key sections:\n",
    "\n",
    "1. **Import Libraries**: Imports necessary libraries for numerical computations and plotting.\n",
    "2. **Define Hyperparameters and Functions**: Sets initial weights, bias, and learning rate, and defines key functions for the logistic regression model.\n",
    "3. **Training Process**: Executes the forward propagation and backward propagation steps, updates weights and bias, and writes iteration data to a CSV file.\n",
    "4. **Visualization**: Plots the error and learning rate across iterations.\n",
    "\n",
    "### 1. Import Libraries\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "```\n",
    "\n",
    "- `numpy`: For numerical computations.\n",
    "- `matplotlib.pyplot`: For plotting the results.\n",
    "- `csv`: For writing the training data to a CSV file.\n",
    "\n",
    "### 2. Define Hyperparameters and Functions\n",
    "\n",
    "**Hyperparameters and Initial Values**:\n",
    "\n",
    "```python\n",
    "x1 = 0.1\n",
    "x2 = 0.3\n",
    "y_target = 0.03\n",
    "\n",
    "w1 = 0.5\n",
    "w2 = 0.2\n",
    "b = 1.83\n",
    "\n",
    "learning_rate = 0.001\n",
    "```\n",
    "\n",
    "- `x1`, `x2`: Input features.\n",
    "- `y_target`: Target value for prediction.\n",
    "- `w1`, `w2`: Initial weights.\n",
    "- `b`: Initial bias.\n",
    "- `learning_rate`: Rate at which weights and bias are updated.\n",
    "\n",
    "**Activation Function (Sigmoid)**:\n",
    "\n",
    "```python\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "```\n",
    "\n",
    "The sigmoid function maps any real-valued number into the (0, 1) interval, which is useful for binary classification problems.\n",
    "\n",
    "**Error Calculation**:\n",
    "\n",
    "```python\n",
    "def calculate_error(y_pred, y_true):\n",
    "    return 0.5 * (y_pred - y_true) ** 2\n",
    "```\n",
    "\n",
    "This function calculates the squared error between the predicted and true values.\n",
    "\n",
    "**Forward Propagation**:\n",
    "\n",
    "```python\n",
    "def forward_propagation(w1, w2, b, x1, x2):\n",
    "    z = w1 * x1 + w2 * x2 + b\n",
    "    y_pred = sigmoid(z)\n",
    "    return y_pred, z\n",
    "```\n",
    "\n",
    "Computes the weighted sum (`z`) and the predicted value (`y_pred`).\n",
    "\n",
    "**Backward Propagation**:\n",
    "\n",
    "```python\n",
    "def backward_propagation(y_pred, y_true, z, x1, x2, w1, w2, b, learning_rate):\n",
    "    g_prime = y_pred * (1 - y_pred)\n",
    "    dz = (y_pred - y_true) * g_prime\n",
    "\n",
    "    dw1 = dz * x1\n",
    "    dw2 = dz * x2\n",
    "    db = dz\n",
    "\n",
    "    w1 -= learning_rate * dw1\n",
    "    w2 -= learning_rate * dw2\n",
    "    b -= learning_rate * db\n",
    "\n",
    "    return w1, w2, b, dw1, dw2, db\n",
    "```\n",
    "\n",
    "Updates the weights and bias using the gradients computed from the error. \n",
    "\n",
    "### 3. Training Process\n",
    "\n",
    "The training process involves multiple iterations of forward and backward propagation, updating weights and bias, and logging the results.\n",
    "\n",
    "```python\n",
    "filename = \"iterations_data.csv\"\n",
    "\n",
    "with open(filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    writer.writerow([\"Iteration\", \"w1\", \"w2\", \"x1\", \"x2\", \"b\", \"Error\", \"GradientError_w1\", \"GradientError_w2\", \"GradientError_b\", \"LearningRate\"])\n",
    "\n",
    "    errors = []\n",
    "    iterations = []\n",
    "    learning_rates = []\n",
    "\n",
    "    for i in range(2):\n",
    "        y_pred, z = forward_propagation(w1, w2, b, x1, x2)\n",
    "        error = calculate_error(y_pred, y_target)\n",
    "        w1, w2, b, dw1, dw2, db = backward_propagation(y_pred, y_target, z, x1, x2, w1, w2, b, learning_rate)\n",
    "        \n",
    "        writer.writerow([i+1, w1, w2, x1, x2, b, error, dw1, dw2, db, learning_rate])\n",
    "        \n",
    "        errors.append(error)\n",
    "        iterations.append(i+1)\n",
    "        learning_rates.append(learning_rate)\n",
    "    \n",
    "    convergence_threshold = 0.0001\n",
    "    max_iterations = 1000\n",
    "    current_iteration = 2\n",
    "\n",
    "    while error > convergence_threshold and current_iteration < max_iterations:\n",
    "        learning_rate = min(learning_rate * 1.1, 0.01)  \n",
    "\n",
    "        y_pred, z = forward_propagation(w1, w2, b, x1, x2)\n",
    "        error = calculate_error(y_pred, y_target)\n",
    "        w1, w2, b, dw1, dw2, db = backward_propagation(y_pred, y_target, z, x1, x2, w1, w2, b, learning_rate)\n",
    "\n",
    "        writer.writerow([current_iteration + 1, w1, w2, x1, x2, b, error, dw1, dw2, db, learning_rate])\n",
    "\n",
    "        errors.append(error)\n",
    "        iterations.append(current_iteration + 1)\n",
    "        learning_rates.append(learning_rate)\n",
    "\n",
    "        current_iteration += 1\n",
    "```\n",
    "5\n",
    "- **Initial Iterations**: Performs two initial iterations and writes the results to a CSV file.\n",
    "- **Convergence Loop**: Continuously updates the learning rate, performs forward and backward propagation, and writes results until the error falls below a threshold or the maximum number of iterations is reached.\n",
    "\n",
    "### 4. Visualization\n",
    "\n",
    "```python\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Error', color=color)\n",
    "ax1.plot(iterations, errors, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Learning Rate', color=color)\n",
    "ax2.plot(iterations, learning_rates, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title(\"Error and Learning Rate vs. Iterations\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- **Error Plot**: Plots the error against the number of iterations.\n",
    "- **Learning Rate Plot**: Plots the learning rate against the number of iterations on a secondary y-axis.\n",
    "\n",
    "### Final Output\n",
    "\n",
    "The final weights, bias, and error after the training process are:\n",
    "\n",
    "```python\n",
    "w1, w2, b, error\n",
    "```\n",
    "\n",
    "### Learning Rate and Error Curve\n",
    "\n",
    "![Figure_1](Figure_1.png)\n",
    "\n",
    "\n",
    "These values represent the optimized parameters.\n",
    "\n",
    "### Summary\n",
    "\n",
    "This implementation demonstrates the process of training a logistic regression model using gradient descent. The model's weights and bias are updated iteratively to minimize the prediction error, and the learning rate is adjusted dynamically to facilitate convergence. The CSV file logs the parameters and errors at each iteration, while the plot provides a visual representation of the error and learning rate trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural-Network-7-Segment-Display\n",
    "\n",
    "### Problem Overview\n",
    "\n",
    "1. **Input Representation**:\n",
    "   Each digit is represented using a 7-segment display with binary encoding for segments `a-g`, as shown in the uploaded table. This will form the input layer of our network.\n",
    "   \n",
    "2. **Output Representation**:\n",
    "   The network will have 10 output nodes (one for each digit, 0 to 9). Each node should output `1` if the corresponding digit is the input; otherwise, it should output `0`.\n",
    "\n",
    "3. **Network Design**:\n",
    "   - **Input Layer**: 7 nodes (corresponding to segments `a` to `g` of the 7-segment display).\n",
    "   - **Hidden Layers**: Two hidden layers with adjustable numbers of neurons (we'll analyze the impact of this in point (i)).\n",
    "   - **Output Layer**: 10 nodes (one-hot encoding for digits 0 to 9).\n",
    "\n",
    "4. **Activation Function**: Sigmoid function (non-linear) for each layer except the output layer. This will allow us to compute gradients smoothly.\n",
    "   \n",
    "5. **Loss Function**: Mean Squared Error (MSE), given by:\n",
    "   $\n",
    "   MSE = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{10} (y_{ij} - \\hat{y}_{ij})^2\n",
    "   $\n",
    "   where $ y_{ij} $ is the true output (1 if the digit is $ i $, 0 otherwise) and $ \\hat{y}_{ij} $ is the predicted output for each of the 10 output neurons.\n",
    "\n",
    "---\n",
    "\n",
    "### Solution Approach\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - **Dataset Creation**: Use the 7-segment code table to create an input-output dataset. Each digit (0-9) will have a binary input vector of 7 segments and a one-hot encoded output vector of 10 values.\n",
    "   - **Training Data**: Since the patterns are fixed, we may create data pairs (input, output) for digits 0-9.\n",
    "   - **Data Augmentation** (if needed): Introduce slight noise to the 7-segment code for robustness (optional).\n",
    "\n",
    "2. **Mathematical Formulation**:\n",
    "\n",
    "   - **Feedforward Pass**:\n",
    "     - Let $ x $ represent the input vector (binary vector of length 7).\n",
    "     - Let $ W^{(1)}, W^{(2)}, W^{(3)} $ represent the weight matrices between the input and first hidden layer, first and second hidden layer, and second hidden layer and output layer, respectively.\n",
    "     - **Hidden Layer 1**: Compute activations $ h^{(1)} = \\sigma(W^{(1)} x + b^{(1)}) $.\n",
    "     - **Hidden Layer 2**: Compute activations $ h^{(2)} = \\sigma(W^{(2)} h^{(1)} + b^{(2)}) $.\n",
    "     - **Output Layer**: Compute the output $ y = \\sigma(W^{(3)} h^{(2)} + b^{(3)}) $.\n",
    "\n",
    "   - **Backpropagation**:\n",
    "     - Compute the loss gradient with respect to the output layer weights, then propagate this gradient back through each layer.\n",
    "     - For each layer $ l $, compute:\n",
    "       $\n",
    "       \\delta^{(l)} = (y^{(l)} - \\hat{y}^{(l)}) \\odot \\sigma'(z^{(l)})\n",
    "       $\n",
    "       where $ \\delta $ represents the error term for layer $ l $, and $ z^{(l)} $ is the linear combination of inputs to layer $ l $.\n",
    "     - Update the weights using the gradients computed with respect to $ W^{(l)} $ for each layer.\n",
    "\n",
    "Backpropagation is the process used to calculate the gradient of the loss function with respect to each weight in the network. In this problem, we have a feedforward neural network with two hidden layers, a sigmoid activation function, and Mean Squared Error (MSE) as the loss function. I'll walk through each step of the backpropagation process mathematically.\n",
    "\n",
    "### Notation and Setup\n",
    "\n",
    "1. **Inputs and Outputs**:\n",
    "   - Let $ x $ be the input vector of length 7 (for the 7-segment display segments).\n",
    "   - The network has two hidden layers with $ H_1 $ and $ H_2 $ neurons respectively.\n",
    "   - The output layer has 10 neurons (one for each digit, 0-9).\n",
    "\n",
    "2. **Weight Matrices and Biases**:\n",
    "   - $ W^{(1)} $: Weight matrix between the input layer and the first hidden layer, of shape $ H_1 \\times 7 $.\n",
    "   - $ W^{(2)} $: Weight matrix between the first and second hidden layer, of shape $ H_2 \\times H_1 $.\n",
    "   - $ W^{(3)} $: Weight matrix between the second hidden layer and the output layer, of shape $ 10 \\times H_2 $.\n",
    "   - $ b^{(1)}, b^{(2)}, b^{(3)} $: Bias vectors for each layer.\n",
    "\n",
    "3. **Activations and Pre-Activations**:\n",
    "   - $ z^{(l)} $: Linear combination (pre-activation) of inputs at layer $ l $.\n",
    "   - $ a^{(l)} $: Activation (post-activation) of neurons at layer $ l $.\n",
    "\n",
    "4. **Activation Function**:\n",
    "   - Sigmoid function: $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $.\n",
    "   - Sigmoid derivative: $ \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) $.\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "1. **Layer 1 (Input to First Hidden Layer)**:\n",
    "   $\n",
    "   z^{(1)} = W^{(1)} x + b^{(1)}\n",
    "   $\n",
    "   $\n",
    "   a^{(1)} = \\sigma(z^{(1)})\n",
    "   $\n",
    "\n",
    "2. **Layer 2 (First Hidden Layer to Second Hidden Layer)**:\n",
    "   $\n",
    "   z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}\n",
    "   $\n",
    "   $\n",
    "   a^{(2)} = \\sigma(z^{(2)})\n",
    "   $\n",
    "\n",
    "3. **Output Layer (Second Hidden Layer to Output Layer)**:\n",
    "   $\n",
    "   z^{(3)} = W^{(3)} a^{(2)} + b^{(3)}\n",
    "   $\n",
    "   $\n",
    "   a^{(3)} = \\sigma(z^{(3)})\n",
    "   $\n",
    "\n",
    "Here, $ a^{(3)} $ is the final output vector of the network, representing the predicted probabilities for each digit.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The Mean Squared Error (MSE) loss is given by:\n",
    "$\n",
    "L = \\frac{1}{10} \\sum_{j=1}^{10} (y_j - a_j^{(3)})^2\n",
    "$\n",
    "where $ y $ is the true output (one-hot encoded vector for the target digit) and $ a^{(3)} $ is the network’s predicted output.\n",
    "\n",
    "### Backpropagation Steps\n",
    "\n",
    "The goal of backpropagation is to compute the gradients of the loss $ L $ with respect to each weight and bias in the network, so that we can update them to minimize the loss.\n",
    "\n",
    "    \n",
    "#### Step 1: Compute the Output Layer Error\n",
    "\n",
    "For each output neuron $ j $ in the output layer:\n",
    "$\n",
    "\\delta^{(3)}_j = \\frac{\\partial L}{\\partial z^{(3)}_j}\n",
    "$\n",
    "Using the chain rule, we get:\n",
    "$\n",
    "\\delta^{(3)}_j = \\frac{\\partial L}{\\partial a^{(3)}_j} \\cdot \\frac{\\partial a^{(3)}_j}{\\partial z^{(3)}_j}\n",
    "$\n",
    "1. **Derivative of Loss w.r.t. $ a^{(3)}_j $**:\n",
    "   $\n",
    "   \\frac{\\partial L}{\\partial a^{(3)}_j} = \\frac{2}{10} (a^{(3)}_j - y_j)\n",
    "   $\n",
    "\n",
    "2. **Derivative of Activation w.r.t. $ z^{(3)}_j $**:\n",
    "   Since $ a^{(3)}_j = \\sigma(z^{(3)}_j) $:\n",
    "   $\n",
    "   \\frac{\\partial a^{(3)}_j}{\\partial z^{(3)}_j} = \\sigma(z^{(3)}_j) (1 - \\sigma(z^{(3)}_j)) = a^{(3)}_j (1 - a^{(3)}_j)\n",
    "   $\n",
    "\n",
    "Combining these, we get:\n",
    "$\n",
    "\\delta^{(3)}_j = \\frac{2}{10} (a^{(3)}_j - y_j) \\cdot a^{(3)}_j (1 - a^{(3)}_j)\n",
    "$\n",
    "\n",
    "#### Step 2: Compute the Second Hidden Layer Error\n",
    "\n",
    "The error at the second hidden layer is calculated by propagating the output layer error backward through the weights $ W^{(3)} $:\n",
    "$\n",
    "\\delta^{(2)} = (W^{(3)})^T \\delta^{(3)} \\odot \\sigma'(z^{(2)})\n",
    "$\n",
    "where $ \\odot $ denotes element-wise multiplication, and $ \\sigma'(z^{(2)}) $ is the derivative of the sigmoid activation at layer 2:\n",
    "$\n",
    "\\sigma'(z^{(2)}) = a^{(2)} \\odot (1 - a^{(2)})\n",
    "$\n",
    "Thus,\n",
    "$\n",
    "\\delta^{(2)} = (W^{(3)})^T \\delta^{(3)} \\odot a^{(2)} (1 - a^{(2)})\n",
    "$\n",
    "\n",
    "#### Step 3: Compute the First Hidden Layer Error\n",
    "\n",
    "Similarly, we propagate the error backward from the second hidden layer to the first hidden layer:\n",
    "$\n",
    "\\delta^{(1)} = (W^{(2)})^T \\delta^{(2)} \\odot \\sigma'(z^{(1)})\n",
    "$\n",
    "where\n",
    "$\n",
    "\\sigma'(z^{(1)}) = a^{(1)} \\odot (1 - a^{(1)})\n",
    "$\n",
    "So,\n",
    "$\n",
    "\\delta^{(1)} = (W^{(2)})^T \\delta^{(2)} \\odot a^{(1)} (1 - a^{(1)})\n",
    "$\n",
    "\n",
    "### Step 4: Gradient Calculation\n",
    "\n",
    "Using the error terms $ \\delta^{(1)}, \\delta^{(2)}, \\delta^{(3)} $, we can now calculate the gradients with respect to each weight matrix and bias vector.\n",
    "\n",
    "1. **Gradients for Output Layer Weights and Biases**:\n",
    "   $\n",
    "   \\frac{\\partial L}{\\partial W^{(3)}} = \\delta^{(3)} (a^{(2)})^T\n",
    "   $\n",
    "   $\n",
    "   \\frac{\\partial L}{\\partial b^{(3)}} = \\delta^{(3)}\n",
    "   $\n",
    "\n",
    "2. **Gradients for Second Hidden Layer Weights and Biases**:\n",
    "   $\n",
    "   \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)} (a^{(1)})^T\n",
    "   $\n",
    "   $\n",
    "   \\frac{\\partial L}{\\partial b^{(2)}} = \\delta^{(2)}\n",
    "   $\n",
    "\n",
    "3. **Gradients for First Hidden Layer Weights and Biases**:\n",
    "   $\n",
    "   \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)} x^T\n",
    "   $\n",
    "   $\n",
    "   \\frac{\\partial L}{\\partial b^{(1)}} = \\delta^{(1)}\n",
    "   $\n",
    "\n",
    "### Step 5: Update Weights and Biases\n",
    "\n",
    "After computing the gradients, we update each weight and bias using gradient descent with learning rate $ \\eta $:\n",
    "$\n",
    "W^{(l)} = W^{(l)} - \\eta \\frac{\\partial L}{\\partial W^{(l)}}\n",
    "$\n",
    "$\n",
    "b^{(l)} = b^{(l)} - \\eta \\frac{\\partial L}{\\partial b^{(l)}}\n",
    "$\n",
    "for each layer $ l $.\n",
    "\n",
    "\n",
    "\n",
    "3. **Convergence Analysis**:\n",
    "   - Plot the loss function (MSE) over iterations to study convergence. Adjust learning rates to observe differences in the rate and stability of convergence.\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - **N-Fold Cross-Validation**: Divide the dataset into N folds and iteratively train on $ N-1 $ folds, evaluating on the remaining fold. Repeat this process N times to compute performance metrics:\n",
    "     - **Accuracy**: Proportion of correct predictions over total predictions.\n",
    "     - **Precision**: $ \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}} $\n",
    "     - **Recall (Sensitivity)**: $ \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}} $\n",
    "     - **Specificity**: $ \\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}} $\n",
    "     - **F-Measure**: Harmonic mean of Precision and Recall.\n",
    "\n",
    "5. **Experiment with Network Hyperparameters**:\n",
    "   - **Learning Rate**: Test different learning rates (e.g., 0.01, 0.1, 0.5) to observe their effects on convergence.\n",
    "   - **Hidden Layers**: Vary the number of hidden neurons and layers to study the trade-offs between network capacity and generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Outline\n",
    "\n",
    "1. **Data Preparation in Python**:\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   \n",
    "   # 7-segment data for digits 0-9\n",
    "   seven_segment_data = {\n",
    "       0: [1, 1, 1, 1, 1, 1, 0],\n",
    "       1: [0, 1, 1, 0, 0, 0, 0],\n",
    "       # Complete this for digits 2-9\n",
    "   }\n",
    "\n",
    "   # One-hot encoded output\n",
    "   labels = np.eye(10)  # 10x10 identity matrix\n",
    "   ```\n",
    "\n",
    "2. **Feedforward and Backpropagation Functions**:\n",
    "   Implement the forward and backpropagation processes with numpy for efficiency. Here's a basic outline:\n",
    "\n",
    "   ```python\n",
    "   def sigmoid(x):\n",
    "       return 1 / (1 + np.exp(-x))\n",
    "\n",
    "   def sigmoid_derivative(x):\n",
    "       return x * (1 - x)\n",
    "\n",
    "   # Initialize weights and biases (randomly)\n",
    "   # Implement feedforward and backpropagation using numpy operations\n",
    "   ```\n",
    "\n",
    "3. **Training and Evaluation Loop**:\n",
    "   Implement a training loop that logs the loss over iterations and evaluates performance using N-fold cross-validation.\n",
    "\n",
    "4. **Plotting Convergence**:\n",
    "   Use `matplotlib` to plot the loss against iterations for each learning rate and network configuration.\n",
    "\n",
    "5. **Cross-Validation**:\n",
    "   Implement cross-validation to compute accuracy, precision, recall, etc., and display the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Data Preparation\n",
    "\n",
    "1. **Prepare the 7-segment data**: Each digit from 0 to 9 is represented as a binary vector based on the state of each segment (on or off).\n",
    "2. **One-hot encoding for labels**: The output for each digit is a one-hot encoded vector of length 10.\n",
    "\n",
    "Here's the code to prepare the data:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 7-segment display encoding for digits 0-9\n",
    "seven_segment_data = {\n",
    "    0: [1, 1, 1, 1, 1, 1, 0],\n",
    "    1: [0, 1, 1, 0, 0, 0, 0],\n",
    "    2: [1, 1, 0, 1, 1, 0, 1],\n",
    "    3: [1, 1, 1, 1, 0, 0, 1],\n",
    "    4: [0, 1, 1, 0, 0, 1, 1],\n",
    "    5: [1, 0, 1, 1, 0, 1, 1],\n",
    "    6: [1, 0, 1, 1, 1, 1, 1],\n",
    "    7: [1, 1, 1, 0, 0, 0, 0],\n",
    "    8: [1, 1, 1, 1, 1, 1, 1],\n",
    "    9: [1, 1, 1, 1, 0, 1, 1]\n",
    "}\n",
    "\n",
    "# Converting the data to arrays\n",
    "inputs = np.array(list(seven_segment_data.values()))\n",
    "labels = np.eye(10)  # One-hot encoded labels for digits 0-9\n",
    "\n",
    "# Combine into a DataFrame for easy visualization (optional)\n",
    "df = pd.DataFrame(inputs, columns=['a', 'b', 'c', 'd', 'e', 'f', 'g'])\n",
    "df['digit'] = range(10)\n",
    "print(df)\n",
    "\n",
    "# The inputs and labels are ready for training\n",
    "```\n",
    "\n",
    "### Step 2: Define Activation Functions and Network Initialization\n",
    "\n",
    "Since we’re using a feedforward neural network, we'll define the sigmoid activation function and initialize the weights and biases.\n",
    "\n",
    "```python\n",
    "# Activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Network architecture\n",
    "input_size = 7\n",
    "hidden_layer1_size = 10\n",
    "hidden_layer2_size = 10\n",
    "output_size = 10\n",
    "\n",
    "# Initialize weights and biases randomly\n",
    "np.random.seed(42)  # For reproducibility\n",
    "weights1 = np.random.randn(input_size, hidden_layer1_size)\n",
    "bias1 = np.random.randn(hidden_layer1_size)\n",
    "weights2 = np.random.randn(hidden_layer1_size, hidden_layer2_size)\n",
    "bias2 = np.random.randn(hidden_layer2_size)\n",
    "weights3 = np.random.randn(hidden_layer2_size, output_size)\n",
    "bias3 = np.random.randn(output_size)\n",
    "```\n",
    "\n",
    "### Step 3: Implement Feedforward and Backpropagation Functions\n",
    "\n",
    "The feedforward function calculates the activations at each layer, and the backpropagation function updates weights based on the calculated gradients.\n",
    "\n",
    "```python\n",
    "# Feedforward function\n",
    "def feedforward(x):\n",
    "    # Layer 1\n",
    "    z1 = np.dot(x, weights1) + bias1\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    # Layer 2\n",
    "    z2 = np.dot(a1, weights2) + bias2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    # Output Layer\n",
    "    z3 = np.dot(a2, weights3) + bias3\n",
    "    output = sigmoid(z3)\n",
    "    return output, a1, a2, z1, z2, z3\n",
    "\n",
    "# Backpropagation function\n",
    "def backpropagation(x, y, a1, a2, output, z1, z2, z3, learning_rate=0.1):\n",
    "    # Output layer error and delta\n",
    "    error_output = output - y\n",
    "    delta_output = error_output * sigmoid_derivative(output)\n",
    "    \n",
    "    # Second hidden layer error and delta\n",
    "    error_hidden2 = np.dot(delta_output, weights3.T)\n",
    "    delta_hidden2 = error_hidden2 * sigmoid_derivative(a2)\n",
    "    \n",
    "    # First hidden layer error and delta\n",
    "    error_hidden1 = np.dot(delta_hidden2, weights2.T)\n",
    "    delta_hidden1 = error_hidden1 * sigmoid_derivative(a1)\n",
    "    \n",
    "    # Gradient descent weight updates\n",
    "    global weights1, weights2, weights3, bias1, bias2, bias3\n",
    "    weights3 -= learning_rate * np.dot(a2.T, delta_output)\n",
    "    bias3 -= learning_rate * delta_output.sum(axis=0)\n",
    "    \n",
    "    weights2 -= learning_rate * np.dot(a1.T, delta_hidden2)\n",
    "    bias2 -= learning_rate * delta_hidden2.sum(axis=0)\n",
    "    \n",
    "    weights1 -= learning_rate * np.dot(x.T, delta_hidden1)\n",
    "    bias1 -= learning_rate * delta_hidden1.sum(axis=0)\n",
    "    \n",
    "    # Return the mean squared error for monitoring\n",
    "    mse = np.mean(error_output**2)\n",
    "    return mse\n",
    "```\n",
    "\n",
    "### Step 4: Training Loop with Loss Plotting\n",
    "\n",
    "This loop performs feedforward and backpropagation across multiple epochs to train the network and plot the loss function.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training parameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.1\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(len(inputs)):\n",
    "        x = inputs[i].reshape(1, -1)  # Input vector for the digit\n",
    "        y = labels[i].reshape(1, -1)  # One-hot encoded target output\n",
    "\n",
    "        # Feedforward\n",
    "        output, a1, a2, z1, z2, z3 = feedforward(x)\n",
    "\n",
    "        # Backpropagation and loss calculation\n",
    "        mse = backpropagation(x, y, a1, a2, output, z1, z2, z3, learning_rate)\n",
    "        epoch_loss += mse\n",
    "\n",
    "    # Record the average loss for this epoch\n",
    "    loss_history.append(epoch_loss / len(inputs))\n",
    "    \n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {epoch_loss / len(inputs):.4f}')\n",
    "\n",
    "# Plotting the loss over epochs\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (Loss)')\n",
    "plt.title('Loss Curve')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Step 5: Evaluation Using Cross-Validation (Optional)\n",
    "\n",
    "For evaluation metrics, we would ideally use a larger dataset with noise to observe performance changes. However, with the limited data here, we’ll skip cross-validation and simply validate by checking accuracy on our fixed patterns.\n",
    "\n",
    "### Summary\n",
    "This approach initializes, trains, and visualizes the network for recognizing digits from 7-segment patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To expand the dataset, we can add slight noise to the binary segment values, flipping a segment on or off randomly. This will help simulate minor imperfections in input and make the model more robust to variations in the input patterns.\n",
    "\n",
    "After generating this noisy dataset, we’ll apply **K-fold cross-validation** to evaluate the model’s performance on multiple splits of the data.\n",
    "\n",
    "\n",
    "### Step 1: Generate Noisy Dataset\n",
    "\n",
    "1. **Introduce Noise**: For each digit pattern, we’ll generate multiple noisy samples by flipping the values of each segment with a small probability. For example, with a 10% probability, each segment could randomly switch from 1 to 0 or vice versa.\n",
    "2. **Create New Samples**: For each original sample, we can generate multiple noisy versions (e.g., 10 per sample).\n",
    "\n",
    "Here’s the code to do that:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = {\n",
    "    'digit': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    'a': [1, 0, 1, 1, 0, 1, 1, 1, 1, 1],\n",
    "    'b': [1, 1, 1, 1, 1, 0, 0, 1, 1, 1],\n",
    "    'c': [1, 1, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "    'd': [1, 0, 1, 1, 0, 1, 1, 0, 1, 1],\n",
    "    'e': [1, 0, 1, 0, 0, 0, 1, 0, 1, 1],\n",
    "    'f': [1, 0, 0, 0, 1, 1, 1, 0, 1, 0],\n",
    "    'g': [0, 0, 1, 1, 1, 1, 1, 0, 1, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to add noise to a binary vector\n",
    "def add_noise(row, noise_level=0.1):\n",
    "    noisy_row = row.copy()\n",
    "    for i in range(1, 8):  # Only apply noise to segments (columns a-g)\n",
    "        if np.random.rand() < noise_level:\n",
    "            noisy_row[i] = 1 - noisy_row[i]  # Flip the bit\n",
    "    return noisy_row\n",
    "\n",
    "# Generate noisy dataset\n",
    "noisy_samples_per_digit = 10  # Number of noisy samples per digit\n",
    "noisy_data = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    for _ in range(noisy_samples_per_digit):\n",
    "        noisy_data.append(add_noise(row.values))\n",
    "\n",
    "# Convert the noisy data to a DataFrame\n",
    "noisy_df = pd.DataFrame(noisy_data, columns=df.columns)\n",
    "print(noisy_df.head())  # Display some samples of the noisy dataset\n",
    "\n",
    "# Save dataset to CSV for inspection if needed\n",
    "noisy_df.to_csv(\"noisy_7_segment_data.csv\", index=False)\n",
    "```\n",
    "\n",
    "This code generates 10 noisy samples per digit, creating a larger dataset with slightly varied patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: K-Fold Cross-Validation\n",
    "\n",
    "We’ll split the noisy dataset into **K folds** (e.g., \\( K = 5 \\)) and then train and evaluate the model on each fold. For each fold, we’ll train on \\( K-1 \\) parts and test on the remaining part.\n",
    "\n",
    "Here's how to perform **K-fold cross-validation** on this dataset:\n",
    "\n",
    "1. **K-Fold Splitting**: Use `KFold` from `sklearn.model_selection` to create training and testing splits.\n",
    "2. **Model Training and Evaluation**: For each fold, train the model on the training split and evaluate it on the test split.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare inputs and labels for training\n",
    "X = noisy_df.iloc[:, 1:].values  # Segment data\n",
    "y = noisy_df['digit'].values     # Labels (digits)\n",
    "\n",
    "# One-hot encode labels\n",
    "Y = np.eye(10)[y]  # Convert digit labels to one-hot encoded vectors\n",
    "\n",
    "# Define K-Fold Cross Validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to track performance metrics\n",
    "fold_accuracies = []\n",
    "\n",
    "# Perform K-Fold Cross Validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    # Split data\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    # Train model\n",
    "    epochs = 200\n",
    "    learning_rate = 0.1\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for i in range(len(X_train)):\n",
    "            x = X_train[i].reshape(1, -1)  # Input vector for the digit\n",
    "            y_true = Y_train[i].reshape(1, -1)  # One-hot encoded target output\n",
    "\n",
    "            # Feedforward\n",
    "            output, a1, a2, z1, z2, z3 = feedforward(x)\n",
    "\n",
    "            # Backpropagation and loss calculation\n",
    "            mse = backpropagation(x, y_true, a1, a2, output, z1, z2, z3, learning_rate)\n",
    "            epoch_loss += mse\n",
    "    \n",
    "    # Evaluate model on the test set\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        x = X_test[i].reshape(1, -1)\n",
    "        output, _, _, _, _, _ = feedforward(x)\n",
    "        predictions.append(np.argmax(output))  # Convert output to digit prediction\n",
    "    \n",
    "    accuracy = accuracy_score(np.argmax(Y_test, axis=1), predictions)\n",
    "    fold_accuracies.append(accuracy)\n",
    "    print(f\"Fold {fold + 1} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate the average accuracy across all folds\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"Average Accuracy across {k} folds: {average_accuracy:.4f}\")\n",
    "\n",
    "# Plot fold accuracies\n",
    "plt.plot(range(1, k + 1), fold_accuracies, marker='o', color='b')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('K-Fold Cross-Validation Accuracy')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "1. **Adding Noise**:\n",
    "   - We create noisy samples for each digit pattern, which helps the model generalize better to variations in input.\n",
    "\n",
    "2. **K-Fold Cross-Validation**:\n",
    "   - We split the data into `k=5` folds.\n",
    "   - For each fold, we train the neural network on \\( K-1 \\) folds and test it on the remaining fold.\n",
    "   - The accuracy for each fold is computed and stored.\n",
    "\n",
    "3. **Evaluating the Model**:\n",
    "   - The average accuracy across all folds is calculated to give a robust estimate of model performance.\n",
    "   - We plot the accuracy for each fold to observe any variations.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "This process expands the dataset with noise, making the neural network more robust. The use of K-fold cross-validation helps assess the model's generalization by testing it on different portions of the dataset. This approach is suitable for verifying the model's robustness and ensuring it can handle real-world variations in 7-segment display patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model more comprehensively, we’ll calculate **Precision**, **Recall (Sensitivity)**, **Specificity**, and **F-Measure** for each fold in addition to accuracy. We’ll also compute the average of each metric across all folds for an overall performance summary.\n",
    "\n",
    "To compute these metrics, we’ll use the following definitions for a multi-class classification problem:\n",
    "\n",
    "1. **Precision**: The number of true positives (correct predictions for a specific digit) divided by the number of true positives and false positives (all predictions for that digit).\n",
    "2. **Recall (Sensitivity)**: The number of true positives divided by the number of true positives and false negatives (all actual instances of that digit).\n",
    "3. **Specificity**: The ability to correctly identify negatives, calculated for each class as the number of true negatives divided by the number of true negatives and false positives.\n",
    "4. **F-Measure**: The harmonic mean of precision and recall.\n",
    "\n",
    "Here’s how to implement it:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Convert labels to binary format for each class\n",
    "y_bin = label_binarize(y, classes=range(10))\n",
    "\n",
    "# Initialize lists to track metrics for each fold\n",
    "fold_accuracies = []\n",
    "fold_precisions = []\n",
    "fold_recalls = []\n",
    "fold_specificities = []\n",
    "fold_f1_scores = []\n",
    "\n",
    "# Perform K-Fold Cross Validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    # Split data\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    # Train model (same as before)\n",
    "    epochs = 200\n",
    "    learning_rate = 0.1\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X_train)):\n",
    "            x = X_train[i].reshape(1, -1)  # Input vector for the digit\n",
    "            y_true = Y_train[i].reshape(1, -1)  # One-hot encoded target output\n",
    "\n",
    "            # Feedforward\n",
    "            output, a1, a2, z1, z2, z3 = feedforward(x)\n",
    "\n",
    "            # Backpropagation and loss calculation\n",
    "            backpropagation(x, y_true, a1, a2, output, z1, z2, z3, learning_rate)\n",
    "\n",
    "    # Evaluate model on the test set\n",
    "    predictions = []\n",
    "    for i in range(len(X_test)):\n",
    "        x = X_test[i].reshape(1, -1)\n",
    "        output, _, _, _, _, _ = feedforward(x)\n",
    "        predictions.append(np.argmax(output))  # Convert output to digit prediction\n",
    "\n",
    "    y_true = np.argmax(Y_test, axis=1)\n",
    "    y_pred = np.array(predictions)\n",
    "    \n",
    "    # Calculate metrics for this fold\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    # Specificity calculation\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    specificity = []\n",
    "    for i in range(10):\n",
    "        tn = np.sum(cm) - (np.sum(cm[i, :]) + np.sum(cm[:, i]) - cm[i, i])\n",
    "        fp = np.sum(cm[:, i]) - cm[i, i]\n",
    "        specificity.append(tn / (tn + fp))\n",
    "    avg_specificity = np.mean(specificity)\n",
    "\n",
    "    # Store metrics for each fold\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_precisions.append(precision)\n",
    "    fold_recalls.append(recall)\n",
    "    fold_specificities.append(avg_specificity)\n",
    "    fold_f1_scores.append(f1)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Specificity: {avg_specificity:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "average_precision = np.mean(fold_precisions)\n",
    "average_recall = np.mean(fold_recalls)\n",
    "average_specificity = np.mean(fold_specificities)\n",
    "average_f1_score = np.mean(fold_f1_scores)\n",
    "\n",
    "print(\"\\n--- Average Metrics Across All Folds ---\")\n",
    "print(f\"Average Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Average Precision: {average_precision:.4f}\")\n",
    "print(f\"Average Recall (Sensitivity): {average_recall:.4f}\")\n",
    "print(f\"Average Specificity: {average_specificity:.4f}\")\n",
    "print(f\"Average F1 Score: {average_f1_score:.4f}\")\n",
    "\n",
    "# Plotting for each fold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, k + 1), fold_accuracies, label=\"Accuracy\", marker='o', color='b')\n",
    "plt.plot(range(1, k + 1), fold_precisions, label=\"Precision\", marker='o', color='g')\n",
    "plt.plot(range(1, k + 1), fold_recalls, label=\"Recall\", marker='o', color='r')\n",
    "plt.plot(range(1, k + 1), fold_specificities, label=\"Specificity\", marker='o', color='purple')\n",
    "plt.plot(range(1, k + 1), fold_f1_scores, label=\"F1 Score\", marker='o', color='orange')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Performance Metrics Across Folds')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation of Additional Code\n",
    "\n",
    "1. **Calculating Metrics**:\n",
    "   - `precision_score`, `recall_score`, and `f1_score` are calculated using `average='macro'` to treat each class equally, suitable for multi-class classification.\n",
    "   - **Specificity**: Specificity is calculated manually using the confusion matrix, as it’s not directly available in `sklearn.metrics` for multi-class problems. Specificity for each class \\( i \\) is calculated as \\( \\frac{TN}{TN + FP} \\), where `TN` (True Negatives) and `FP` (False Positives) are derived from the confusion matrix.\n",
    "   \n",
    "2. **Storing Fold Metrics**:\n",
    "   - Each fold’s metrics are stored in respective lists, allowing us to compute the average of each metric across all folds at the end.\n",
    "\n",
    "3. **Visualization**:\n",
    "   - The plot shows how each metric varies across the folds, providing a visual representation of the model's performance consistency.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "The above code gives the following:\n",
    "- **Accuracy**, **Precision**, **Recall**, **Specificity**, and **F1 Score** for each fold.\n",
    "- **Average metrics** across all folds, offering an overall evaluation of model performance.\n",
    "- **Plot** displaying the consistency and variance of these metrics across the folds.\n",
    "\n",
    "This approach provides a well-rounded assessment of the model’s performance on noisy data, ensuring its robustness in recognizing digits on a 7-segment display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "### Step 1: Load and Preprocess Data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load CSV dataset\n",
    "data = pd.read_csv(\"alphabet_dataset.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.iloc[:, :-26].values  # 25 binary values per row\n",
    "y = data.iloc[:, -26:].values  # One-hot encoded labels (26 columns)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### Step 2: Define the Feedforward Neural Network Model\n",
    "\n",
    "We’ll create a model with:\n",
    "- **Two hidden layers** (with 32 and 16 neurons respectively)\n",
    "- **Sigmoid activation** function\n",
    "- **MSE loss** function for training\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(32, input_dim=25, activation='sigmoid'),  # First hidden layer with 32 neurons\n",
    "    Dense(16, activation='sigmoid'),                # Second hidden layer with 16 neurons\n",
    "    Dense(26, activation='sigmoid')                 # Output layer with 26 neurons for A-Z classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "### Step 3: Train the Model\n",
    "\n",
    "We’ll train the model using the **training set** and monitor the loss and accuracy during training.\n",
    "\n",
    "```python\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.1)\n",
    "```\n",
    "\n",
    "### Step 4: Evaluate the Model\n",
    "\n",
    "After training, we evaluate the model on the test set and calculate metrics like **accuracy**, **precision**, **recall**, and **F1 score**.\n",
    "\n",
    "```python\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions and labels to class labels from one-hot encoding\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "```\n",
    "\n",
    "### Step 5: Plot Training Loss vs. Epochs\n",
    "\n",
    "To analyze convergence, we’ll plot the training loss and accuracy over epochs.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy Over Epochs')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Step 6: Cross-Validation and Metrics Calculation\n",
    "\n",
    "For **N-Fold Cross-Validation**, you can wrap the model training in a loop, splitting the data for each fold. Below is an example using **5-Fold Cross Validation**:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Initialize 5-Fold Cross Validator\n",
    "kf = KFold(n_splits=5)\n",
    "fold_accuracy, fold_precision, fold_recall, fold_f1 = [], [], [], []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    # Create train/val split for this fold\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "    \n",
    "    # Define and train the model for each fold\n",
    "    model = Sequential([\n",
    "        Dense(32, input_dim=25, activation='sigmoid'),\n",
    "        Dense(16, activation='sigmoid'),\n",
    "        Dense(26, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    model.fit(X_train_fold, y_train_fold, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = model.predict(X_val_fold)\n",
    "    y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "    y_val_classes = np.argmax(y_val_fold, axis=1)\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    fold_accuracy.append(accuracy_score(y_val_classes, y_val_pred_classes))\n",
    "    fold_precision.append(precision_score(y_val_classes, y_val_pred_classes, average='weighted'))\n",
    "    fold_recall.append(recall_score(y_val_classes, y_val_pred_classes, average='weighted'))\n",
    "    fold_f1.append(f1_score(y_val_classes, y_val_pred_classes, average='weighted'))\n",
    "\n",
    "# Average metrics across folds\n",
    "print(f\"Cross-Validated Accuracy: {np.mean(fold_accuracy)}\")\n",
    "print(f\"Cross-Validated Precision: {np.mean(fold_precision)}\")\n",
    "print(f\"Cross-Validated Recall: {np.mean(fold_recall)}\")\n",
    "print(f\"Cross-Validated F1 Score: {np.mean(fold_f1)}\")\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Model Design**: A feedforward neural network with two hidden layers and sigmoid activation for each layer.\n",
    "2. **Training and Loss Calculation**: Using Mean Squared Error as the loss function.\n",
    "3. **Hyperparameter Tuning**: Possible adjustments include the number of neurons, batch size, and learning rate.\n",
    "4. **Cross-Validation**: 5-Fold Cross-Validation to ensure model stability and performance consistency.\n",
    "5. **Metrics and Convergence Analysis**: Accuracy, precision, recall, F1 score, and plotting the convergence.\n",
    "\n",
    "This code provides a full end-to-end approach for building a neural network to recognize alphabets based on a 5x5 grid representation. Adjust hyperparameters as needed for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
